{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# QLoRA Fine-Tuning for Policy Compliance\n",
        "\n",
        "Fine-tunes Llama 3.1 8B on policy data. Runtime: 2-3 hours on T4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q torch transformers accelerate peft bitsandbytes trl datasets sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import os\n",
        "os.environ['ACCELERATE_MIXED_PRECISION'] = 'no'\n",
        "print(f'CUDA: {torch.cuda.is_available()}')\n",
        "if torch.cuda.is_available(): print(f'GPU: {torch.cuda.get_device_name(0)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.colab import files\n",
        "import json\n",
        "uploaded = files.upload()\n",
        "DATA_FILE = list(uploaded.keys())[0]\n",
        "print(f'Uploaded: {DATA_FILE}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import torch\n",
        "\n",
        "MODEL_NAME = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_compute_dtype=torch.float32,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = 'right'\n",
        "\n",
        "print('Loading model...')\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=bnb_config, device_map='auto', torch_dtype=torch.float32)\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Force all non-quantized params to float32\n",
        "for name, param in model.named_parameters():\n",
        "    if param.dtype == torch.bfloat16:\n",
        "        param.data = param.data.to(torch.float32)\n",
        "\n",
        "print('Model loaded!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure LoRA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=64, lora_alpha=128, lora_dropout=0.05, bias='none',\n",
        "    task_type='CAUSAL_LM',\n",
        "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Trainable params: {trainable:,}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from datasets import Dataset\n",
        "import json\n",
        "\n",
        "data = []\n",
        "with open(DATA_FILE, 'r') as f:\n",
        "    for line in f:\n",
        "        if line.strip(): data.append(json.loads(line))\n",
        "print(f'Loaded {len(data)} examples')\n",
        "\n",
        "PROMPT = '''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are a compliance assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{q}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "{a}<|eot_id|>'''\n",
        "\n",
        "def fmt(ex):\n",
        "    q = ex.get('question', ex.get('instruction', ''))\n",
        "    a = ex.get('answer', ex.get('output', ''))\n",
        "    return {'text': PROMPT.format(q=q, a=a)}\n",
        "\n",
        "dataset = Dataset.from_list([fmt(d) for d in data])\n",
        "split = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_ds, eval_ds = split['train'], split['test']\n",
        "print(f'Train: {len(train_ds)}, Eval: {len(eval_ds)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "import gc\n",
        "\n",
        "# Clear memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Tokenize data with shorter length\n",
        "def tokenize(example):\n",
        "    return tokenizer(example['text'], truncation=True, max_length=128, padding='max_length')\n",
        "\n",
        "train_tokenized = train_ds.map(tokenize, remove_columns=['text'])\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir='./policy-llama',\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type='cosine',\n",
        "    optim='adamw_bnb_8bit',\n",
        "    logging_steps=25,\n",
        "    eval_strategy='no',\n",
        "    save_strategy='epoch',\n",
        "    bf16=False,\n",
        "    fp16=False,\n",
        "    report_to='none',\n",
        "    dataloader_pin_memory=False,\n",
        "    gradient_checkpointing=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_tokenized,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "print('Ready to train!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "trainer.save_model('./policy-llama/final')\n",
        "tokenizer.save_pretrained('./policy-llama/final')\n",
        "print('Saved!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!zip -r policy-llama.zip ./policy-llama/final\n",
        "files.download('policy-llama.zip')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}