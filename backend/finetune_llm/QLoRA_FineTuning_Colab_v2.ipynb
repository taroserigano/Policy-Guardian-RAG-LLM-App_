{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c95e987",
   "metadata": {},
   "source": [
    "# QLoRA Fine-Tuning for Policy Compliance LLM\n",
    "\n",
    "This notebook fine-tunes Llama 3.1 8B using QLoRA (4-bit quantization) on policy compliance training data.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU (T4 free tier works)\n",
    "- HuggingFace account with Llama 3.1 access\n",
    "- Training data (upload `training_data_augmented.jsonl`)\n",
    "\n",
    "**Runtime:** ~2-3 hours on T4 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397182e2",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dd6454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers accelerate peft bitsandbytes trl datasets sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e122c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e1e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace (required for Llama 3.1)\n",
    "from huggingface_hub import login\n",
    "login()  # Enter your HuggingFace token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521ebb05",
   "metadata": {},
   "source": [
    "## 2. Upload Training Data\n",
    "\n",
    "Upload your `training_data_augmented.jsonl` file (546 examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6202dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import json\n",
    "\n",
    "# Upload training data\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Check uploaded file\n",
    "for filename in uploaded.keys():\n",
    "    print(f\"Uploaded: {filename}\")\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        print(f\"Total examples: {len(lines)}\")\n",
    "        print(f\"Sample: {json.loads(lines[0])}\")\n",
    "    DATA_FILE = filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc1d8dd",
   "metadata": {},
   "source": [
    "## 3. Load Model with 4-bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e161b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# 4-bit quantization config (QLoRA)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"Loading model with 4-bit quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Prepare for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "print(f\"Model loaded! Parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92196df",
   "metadata": {},
   "source": [
    "## 4. Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251696ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=64,                    # LoRA rank\n",
    "    lora_alpha=128,          # Scaling factor\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable: {trainable_params:,} ({100 * trainable_params / all_params:.2f}%)\")\n",
    "print(f\"Total: {all_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfc0591",
   "metadata": {},
   "source": [
    "## 5. Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd98d5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "# Load training data\n",
    "data = []\n",
    "with open(DATA_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(data)} examples\")\n",
    "\n",
    "# Prompt template for Llama 3.1\n",
    "PROMPT_TEMPLATE = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a compliance assistant specializing in corporate policies. You provide accurate, helpful answers about policy requirements, procedures, and best practices. Always cite specific policy requirements when applicable.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{response}<|eot_id|>\"\"\"\n",
    "\n",
    "def format_example(example):\n",
    "    instruction = example.get('question', example.get('instruction', ''))\n",
    "    response = example.get('answer', example.get('output', ''))\n",
    "    return {\"text\": PROMPT_TEMPLATE.format(instruction=instruction, response=response)}\n",
    "\n",
    "# Create dataset\n",
    "formatted_data = [format_example(d) for d in data]\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "# Split into train/eval\n",
    "split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split['train']\n",
    "eval_dataset = split['test']\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Eval: {len(eval_dataset)}\")\n",
    "print(f\"\\nSample:\\n{train_dataset[0]['text'][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcfedcc",
   "metadata": {},
   "source": [
    "## 6. Train with QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70808b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./policy-llama-qlora\",\n",
    "    \n",
    "    # Batch settings (adjust for your GPU memory)\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,  # Effective batch = 16\n",
    "    \n",
    "    # Training duration\n",
    "    num_train_epochs=3,\n",
    "    \n",
    "    # Learning rate\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # Optimization\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    max_grad_norm=0.3,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    # Evaluation\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    "    # Memory optimization\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # Misc\n",
    "    seed=42,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"Trainer ready! Starting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbf2c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271ff4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained adapter\n",
    "trainer.save_model(\"./policy-llama-qlora/final\")\n",
    "tokenizer.save_pretrained(\"./policy-llama-qlora/final\")\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f017251",
   "metadata": {},
   "source": [
    "## 7. Test the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c8c9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model\n",
    "def generate_response(prompt, max_new_tokens=256):\n",
    "    inputs = tokenizer(\n",
    "        f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a compliance assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What are the key requirements for handling personal data?\",\n",
    "    \"What should I do if I suspect a security breach?\",\n",
    "    \"Can I use personal cloud storage for work files?\",\n",
    "    \"How long should confidential information be protected after leaving the company?\",\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Q: {q}\")\n",
    "    response = generate_response(q)\n",
    "    # Extract just the assistant's response\n",
    "    if \"assistant\" in response:\n",
    "        response = response.split(\"assistant\")[-1].strip()\n",
    "    print(f\"A: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b882feef",
   "metadata": {},
   "source": [
    "## 8. Merge Adapter & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceda253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Load base model (full precision for merging)\n",
    "print(\"Loading base model for merging...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Load adapter and merge\n",
    "print(\"Merging adapter...\")\n",
    "merged_model = PeftModel.from_pretrained(base_model, \"./policy-llama-qlora/final\")\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "# Save merged model\n",
    "print(\"Saving merged model...\")\n",
    "merged_model.save_pretrained(\"./policy-llama-merged\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"./policy-llama-merged\")\n",
    "\n",
    "print(\"Merged model saved to ./policy-llama-merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c8335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the merged model\n",
    "!zip -r policy-llama-merged.zip ./policy-llama-merged\n",
    "files.download('policy-llama-merged.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc0a2b",
   "metadata": {},
   "source": [
    "## 9. Convert to GGUF (for Ollama)\n",
    "\n",
    "Run this section to convert to GGUF format for use with Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bbb460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install llama.cpp for GGUF conversion\n",
    "!git clone https://github.com/ggerganov/llama.cpp\n",
    "!cd llama.cpp && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d558b984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to GGUF\n",
    "!python llama.cpp/convert_hf_to_gguf.py ./policy-llama-merged --outfile policy-compliance-llm-f16.gguf --outtype f16\n",
    "\n",
    "print(\"GGUF file created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b4c7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download GGUF file\n",
    "files.download('policy-compliance-llm-f16.gguf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8765a3",
   "metadata": {},
   "source": [
    "## 10. Create Ollama Model (Local)\n",
    "\n",
    "After downloading the GGUF file, run these commands locally:\n",
    "\n",
    "```bash\n",
    "# Create Modelfile\n",
    "cat > Modelfile << 'EOF'\n",
    "FROM ./policy-compliance-llm-f16.gguf\n",
    "\n",
    "PARAMETER temperature 0.7\n",
    "PARAMETER top_p 0.9\n",
    "PARAMETER num_ctx 4096\n",
    "\n",
    "SYSTEM \"\"\"You are a compliance assistant specializing in corporate policies. You provide accurate, helpful answers about policy requirements, procedures, and best practices. Always cite specific policy requirements when applicable.\"\"\"\n",
    "EOF\n",
    "\n",
    "# Create Ollama model\n",
    "ollama create policy-compliance-llm -f Modelfile\n",
    "\n",
    "# Test it\n",
    "ollama run policy-compliance-llm \"What are the key data privacy requirements?\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
