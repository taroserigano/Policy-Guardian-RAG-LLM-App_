import json

notebook = {
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {"provenance": [], "gpuType": "T4"},
        "kernelspec": {"name": "python3", "display_name": "Python 3"},
        "accelerator": "GPU"
    },
    "cells": [
        {"cell_type": "markdown", "metadata": {}, "source": ["# QLoRA Fine-Tuning for Policy Compliance\n", "\n", "Fine-tunes Llama 3.1 8B on policy data. Runtime: 2-3 hours on T4."]},
        {"cell_type": "markdown", "metadata": {}, "source": ["## Mount Google Drive (to save checkpoints)"]},
        {"cell_type": "code", "metadata": {}, "source": [
            "from google.colab import drive\n",
            "drive.mount('/content/drive')\n",
            "import os\n",
            "os.makedirs('/content/drive/MyDrive/policy-llama', exist_ok=True)\n",
            "print('Drive mounted!')"
        ], "execution_count": None, "outputs": []},
        {"cell_type": "code", "metadata": {}, "source": ["!pip install -q torch transformers accelerate peft bitsandbytes trl datasets sentencepiece"], "execution_count": None, "outputs": []},
        {"cell_type": "code", "metadata": {}, "source": ["import torch\n", "import os\n", "os.environ['ACCELERATE_MIXED_PRECISION'] = 'no'\n", "print(f'CUDA: {torch.cuda.is_available()}')\n", "if torch.cuda.is_available(): print(f'GPU: {torch.cuda.get_device_name(0)}')"], "execution_count": None, "outputs": []},
        {"cell_type": "code", "metadata": {}, "source": ["from huggingface_hub import login\n", "login()"], "execution_count": None, "outputs": []},
        {"cell_type": "markdown", "metadata": {}, "source": ["## Upload Training Data"]},
        {"cell_type": "code", "metadata": {}, "source": ["from google.colab import files\n", "import json\n", "uploaded = files.upload()\n", "DATA_FILE = list(uploaded.keys())[0]\n", "print(f'Uploaded: {DATA_FILE}')"], "execution_count": None, "outputs": []},
        {"cell_type": "markdown", "metadata": {}, "source": ["## Load Model"]},
        {"cell_type": "code", "metadata": {}, "source": [
            "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
            "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
            "import torch\n",
            "\n",
            "MODEL_NAME = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
            "\n",
            "bnb_config = BitsAndBytesConfig(\n",
            "    load_in_4bit=True,\n",
            "    bnb_4bit_quant_type='nf4',\n",
            "    bnb_4bit_compute_dtype=torch.float32,\n",
            "    bnb_4bit_use_double_quant=True,\n",
            ")\n",
            "\n",
            "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
            "tokenizer.pad_token = tokenizer.eos_token\n",
            "tokenizer.padding_side = 'right'\n",
            "\n",
            "print('Loading model...')\n",
            "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=bnb_config, device_map='auto', torch_dtype=torch.float32)\n",
            "model = prepare_model_for_kbit_training(model)\n",
            "model.gradient_checkpointing_enable()\n",
            "\n",
            "# Force all non-quantized params to float32\n",
            "for name, param in model.named_parameters():\n",
            "    if param.dtype == torch.bfloat16:\n",
            "        param.data = param.data.to(torch.float32)\n",
            "\n",
            "print('Model loaded!')"
        ], "execution_count": None, "outputs": []},
        {"cell_type": "markdown", "metadata": {}, "source": ["## Configure LoRA"]},
        {"cell_type": "code", "metadata": {}, "source": [
            "lora_config = LoraConfig(\n",
            "    r=64, lora_alpha=128, lora_dropout=0.05, bias='none',\n",
            "    task_type='CAUSAL_LM',\n",
            "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
            ")\n",
            "model = get_peft_model(model, lora_config)\n",
            "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "print(f'Trainable params: {trainable:,}')"
        ], "execution_count": None, "outputs": []},
        {"cell_type": "markdown", "metadata": {}, "source": ["## Prepare Data"]},
        {"cell_type": "code", "metadata": {}, "source": [
            "from datasets import Dataset\n",
            "import json\n",
            "\n",
            "data = []\n",
            "with open(DATA_FILE, 'r') as f:\n",
            "    for line in f:\n",
            "        if line.strip(): data.append(json.loads(line))\n",
            "print(f'Loaded {len(data)} examples')\n",
            "\n",
            "PROMPT = '''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "You are a compliance assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "{q}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "{a}<|eot_id|>'''\n",
            "\n",
            "def fmt(ex):\n",
            "    q = ex.get('question', ex.get('instruction', ''))\n",
            "    a = ex.get('answer', ex.get('output', ''))\n",
            "    return {'text': PROMPT.format(q=q, a=a)}\n",
            "\n",
            "dataset = Dataset.from_list([fmt(d) for d in data])\n",
            "split = dataset.train_test_split(test_size=0.1, seed=42)\n",
            "train_ds, eval_ds = split['train'], split['test']\n",
            "print(f'Train: {len(train_ds)}, Eval: {len(eval_ds)}')"
        ], "execution_count": None, "outputs": []},
        {"cell_type": "markdown", "metadata": {}, "source": ["## Train"]},
        {"cell_type": "code", "metadata": {}, "source": [
            "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
            "import gc\n",
            "\n",
            "# Clear memory\n",
            "gc.collect()\n",
            "torch.cuda.empty_cache()\n",
            "\n",
            "# Tokenize data with shorter length\n",
            "def tokenize(example):\n",
            "    return tokenizer(example['text'], truncation=True, max_length=128, padding='max_length')\n",
            "\n",
            "train_tokenized = train_ds.map(tokenize, remove_columns=['text'])\n",
            "\n",
            "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
            "\n",
            "args = TrainingArguments(\n",
            "    output_dir='/content/drive/MyDrive/policy-llama/checkpoints',\n",
            "    per_device_train_batch_size=1,\n",
            "    gradient_accumulation_steps=16,\n",
            "    num_train_epochs=3,\n",
            "    learning_rate=2e-4,\n",
            "    warmup_ratio=0.03,\n",
            "    lr_scheduler_type='cosine',\n",
            "    optim='adamw_bnb_8bit',\n",
            "    logging_steps=25,\n",
            "    eval_strategy='no',\n",
            "    save_strategy='epoch',\n",
            "    bf16=False,\n",
            "    fp16=False,\n",
            "    report_to='none',\n",
            "    dataloader_pin_memory=False,\n",
            "    gradient_checkpointing=True,\n",
            ")\n",
            "\n",
            "trainer = Trainer(\n",
            "    model=model,\n",
            "    args=args,\n",
            "    train_dataset=train_tokenized,\n",
            "    data_collator=data_collator,\n",
            ")\n",
            "print('Ready to train!')"
        ], "execution_count": None, "outputs": []},
        {"cell_type": "code", "metadata": {}, "source": ["trainer.train()"], "execution_count": None, "outputs": []},
        {"cell_type": "code", "metadata": {}, "source": [
            "# Save final model to Drive\n",
            "trainer.save_model('/content/drive/MyDrive/policy-llama/final')\n",
            "tokenizer.save_pretrained('/content/drive/MyDrive/policy-llama/final')\n",
            "print('Saved to Google Drive!')"
        ], "execution_count": None, "outputs": []},
        {"cell_type": "markdown", "metadata": {}, "source": ["## Merge Adapter into Base Model"]},
        {"cell_type": "code", "metadata": {}, "source": [
            "# Clear GPU memory\n",
            "import gc\n",
            "del model, trainer\n",
            "gc.collect()\n",
            "torch.cuda.empty_cache()\n",
            "print('Memory cleared')"
        ], "execution_count": None, "outputs": []},
        {"cell_type": "code", "metadata": {}, "source": [
            "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "from peft import PeftModel\n",
            "import torch\n",
            "\n",
            "print('Loading base model...')\n",
            "base_model = AutoModelForCausalLM.from_pretrained(\n",
            "    'meta-llama/Meta-Llama-3.1-8B-Instruct',\n",
            "    device_map='cpu',\n",
            "    torch_dtype=torch.float16,\n",
            ")\n",
            "\n",
            "print('Loading adapter...')\n",
            "model_with_adapter = PeftModel.from_pretrained(base_model, '/content/drive/MyDrive/policy-llama/final')\n",
            "\n",
            "print('Merging adapter into base...')\n",
            "merged_model = model_with_adapter.merge_and_unload()\n",
            "\n",
            "print('Saving merged model...')\n",
            "merged_model.save_pretrained('./policy-llama-merged', safe_serialization=True)\n",
            "tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/policy-llama/final')\n",
            "tokenizer.save_pretrained('./policy-llama-merged')\n",
            "print('Merged model saved!')"
        ], "execution_count": None, "outputs": []},
        {"cell_type": "markdown", "metadata": {}, "source": ["## Convert to GGUF for Ollama"]},
        {"cell_type": "code", "metadata": {}, "source": [
            "# Install llama.cpp conversion tools\n",
            "!pip install -q gguf"
        ], "execution_count": None, "outputs": []},
        {"cell_type": "code", "metadata": {}, "source": [
            "# Clone llama.cpp for conversion script\n",
            "!git clone https://github.com/ggerganov/llama.cpp.git\n",
            "!pip install -q -r llama.cpp/requirements.txt"
        ], "execution_count": None, "outputs": []},
        {"cell_type": "code", "metadata": {}, "source": [
            "# Convert to FP16 GGUF first\n",
            "!python llama.cpp/convert_hf_to_gguf.py ./policy-llama-merged --outtype f16 --outfile policy-compliance-llm-f16.gguf"
        ], "execution_count": None, "outputs": []},
        {"cell_type": "code", "metadata": {}, "source": [
            "# Quantize to Q4_K_M (recommended for Ollama)\n",
            "!cd llama.cpp && make -j\n",
            "!./llama.cpp/llama-quantize policy-compliance-llm-f16.gguf policy-compliance-llm-Q4_K_M.gguf Q4_K_M"
        ], "execution_count": None, "outputs": []},
        {"cell_type": "markdown", "metadata": {}, "source": ["## Download GGUF Model"]},
        {"cell_type": "code", "metadata": {}, "source": [
            "from google.colab import files\n",
            "import os\n",
            "\n",
            "# Check file size\n",
            "size_mb = os.path.getsize('policy-compliance-llm-Q4_K_M.gguf') / (1024*1024)\n",
            "print(f'GGUF size: {size_mb:.1f} MB')\n",
            "\n",
            "# Download (will take a few minutes)\n",
            "print('Starting download...')\n",
            "files.download('policy-compliance-llm-Q4_K_M.gguf')"
        ], "execution_count": None, "outputs": []},
    ]
}

with open('QLoRA_Colab.ipynb', 'w', encoding='utf-8') as f:
    json.dump(notebook, f, indent=2)
print('Created QLoRA_Colab.ipynb!')
